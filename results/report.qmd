---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "List names here"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

Task 1

When looking at the raw distributions of the biomarker-raw.csv file some interesting findings occurred. As seen from the graphs below, we can see that the distribution all the distributions are all skewed right. Some of them vary on the severity of the skewness but they all produce a right skew nonetheless. This makes sense why they would need to be log transformed. Log transformations can introduce normality into the data distributions. This is important because of how skewed our data is. By log transforming, we are able to normalize the data which allows us to perform the t-tests on the data that require normality to be accurate in their analysis.

```{r, warning = FALSE}
# Load necessary libraries
library(ggplot2)
library(tidyr)
library(dplyr)

# Read the data
data <- read.csv("biomarker-raw.csv")

# Select a subset of columns for proteins (skip the first two columns)
protein_columns <- colnames(data)[3:length(colnames(data))]

# Randomly select 5 protein columns
set.seed(123) # Set seed for reproducibility
selected_proteins <- sample(protein_columns, 5)

# Convert selected columns to numeric and handle any issues with non-numeric data
data[selected_proteins] <- lapply(data[selected_proteins], function(x) as.numeric(as.character(x)))

# Filter to keep only rows without NA values in selected proteins
protein_data <- data %>% select(all_of(selected_proteins)) %>% drop_na()

# Reshape data for ggplot
protein_data_long <- protein_data %>% 
  pivot_longer(cols = everything(), names_to = "Protein", values_to = "Raw_Value") %>%
  mutate(Raw_Value = as.numeric(Raw_Value))  # Ensure Raw_Value is numeric

# Plot distributions using ggplot2
ggplot(protein_data_long, aes(x = Raw_Value)) +
  geom_histogram(bins = 20, fill = "skyblue", color = "black") +
  facet_wrap(~ Protein, scales = "free_x") +
  labs(title = "Distribution of Raw Values for Selected Proteins",
       x = "Raw Value", y = "Frequency") +
  theme_minimal()
```

This makes sense why they would need to be log transformed. Log transformations can introduce normality into the data distributions. This is important because of how skewed our data is. By log transforming, we are able to normalize the data which allows us to perform the t-tests on the data that require normality to be accurate in their analysis. Let us now look at how the log transformed distributions for another 5 sampled proteins turns out.

```{r}

# temporarily removing the outlier trimming from preprocessing.R

library(tidyverse)

# get names
var_names <- read_csv('biomarker-raw.csv', 
                     col_names = F,
                     show_col_types = F,
                     n_max = 2, 
                     col_select = -(1:2)) %>%
  t() %>%
  as_tibble() %>%
  rename(name = V1, 
         abbreviation = V2) %>%
  na.omit()

# function for trimming outliers (good idea??)
trim <- function(x, .at){
  x[abs(x) > .at] <- sign(x[abs(x) > .at])*.at
  return(x)
}

# read in data
biomarker_clean <- read_csv('biomarker-raw.csv', 
         skip = 2,
         show_col_types = F,
         col_select = -2L,
         col_names = c('group', 
                       'empty',
                       pull(var_names, abbreviation),
                       'ados'),
         na = c('-', '')) %>%
  filter(!is.na(group)) %>%
  # log transform, center and scale, and trim
  mutate(across(.cols = -c(group, ados), 
                ~ trim(scale(log10(.x))[, 1], .at = 3))) %>%
  # reorder columns
  select(group, ados, everything())


sampled_proteins <- biomarker_clean %>%
  select(sample(names(.), 5)) %>%
  pivot_longer(cols = everything(), names_to = "Protein", values_to = "Transformed_Value")

# Plot histograms for each selected protein using facet_wrap (Transformed Values)
ggplot(sampled_proteins, aes(x = Transformed_Value)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +
  facet_wrap(~ Protein, scales = "free") +
  labs(title = "Histograms of Transformed Values for Selected Proteins",
       x = "Transformed Value",
       y = "Frequency") +
  theme_minimal()
```

Here, we can see that the values are much easier to analyze and deal with. They are mostly normally distributed now and their values are ranging between -2 and 2 instead of more sporadic values that populated the histograms with the raw csv file.

Task 2

### Methodlogical variations

Task 3

Results of unmodified analysis:

| Metric      | Estimator | Estimate |
|-------------|-----------|----------|
| sensitivity | binary    | 0.8750   |
| specificity | binary    | 0.8000   |
| accuracy    | binary    | 0.8387   |
| roc_auc     | binary    | 0.9083   |

**Modification 1:** Carrying our selection procedure using a training partition.

In the original analysis, the t-test and random forest methods of selecting important proteins were conducted using the entire biomarker dataset. For the first modification, proteins were selected by partioning the dataset into training and testing groups before analysis, then using training data to select proteins and evaluating the resulting panel's accuracy using testing data.

Below are the results of this modification (the difference column represents the change in each metric from the original method):

| Metric      | Estimator | Estimate | Difference |
|-------------|-----------|----------|------------|
| sensitivity | binary    | 0.7647   | -0.1103    |
| specificity | binary    | 0.7857   | -0.0143    |
| accuracy    | binary    | 0.7741   | -0.0646    |
| roc_auc     | binary    | 0.8067   | -0.1016    |

As we can see, each metric used to evaluate the accuracy of our classifier became worse. Both sensitivity (% of true positives) and roc_auc (a measure of true positive rate and false positive rate ) decreased by over 10 percentage points, while specificity (% of true negatives) and accuracy (% correct) decreased by modest amounts. Thus it seems that partitioning the data prior to conducting analysis in this scenario did not improve results, which is not too surprising considering the modified models were given less data to train on than the unmodified ones. Had more observations been in the biomarker dataset, the results may have more closely aligned. Furthermore, the partitioning of the data is done randomly, so perhaps a different seed would have altered the results.

**Modification 2:** Selecting 20 predictive proteins using each selection method.

While the top 10 predictive proteins were selected from each method during the in-class analysis, we will see if selecting 20 proteins instead will help the classifier's accuracy. Below are the results of carrying out this modification:

| Method      | Estimator | Estimate | Difference |
|-------------|-----------|----------|------------|
| sensitivity | Binary    | 0.812    | -0.063     |
| specificity | Binary    | 0.867    | 0.067      |
| accuracy    | Binary    | 0.839    | 0.0003     |
| roc_auc     | Binary    | 0.946    | 0.0377     |

Based on the results of three of the four metrics, selecting 20 of the most important proteins from each selection method improved the classification accuracy, albeit by relatively small margins. Sensitivity was the only metric included in our set that showed a decline in performance when compared to the original classifier, while the overall accuracy increased by 0.03%. In this scenario, a doubling of the amount of top predictive proteins selected from the multiple testing and random forest methods slightly improved results, but it's unclear whether further increases in this amount would help or harm predictive accuracy, as well as what the perfect amount to select from each method would be.

**Modification 3:** Using a fuzzy (instead of hard) intersection to combine the sets of proteins chosen by each selection method.

### Improved classifier

Task 4
