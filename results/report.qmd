---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "List names here"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

Tasks 1-2

### Methodlogical variations

Task 3

Results of unmodified analysis:

| Metric      | Estimator | Estimate |
|-------------|-----------|----------|
| sensitivity | binary    | 0.8750   |
| specificity | binary    | 0.8000   |
| accuracy    | binary    | 0.8387   |
| roc_auc     | binary    | 0.9083   |

**Modification 1:** Carrying our selection procedure using a training partition.

In the original analysis, the t-test and random forest methods of selecting important proteins were conducted using the entire biomarker dataset. For the first modification, proteins were selected by partioning the dataset into training and testing groups before analysis, then using training data to select proteins and evaluating the resulting panel's accuracy using testing data.

Below are the results of this modification (the difference column represents the change in each metric from the original method):

| Metric      | Estimator | Estimate | Difference |
|-------------|-----------|----------|------------|
| sensitivity | binary    | 0.7647   | -0.1103    |
| specificity | binary    | 0.7857   | -0.0143    |
| accuracy    | binary    | 0.7741   | -0.0646    |
| roc_auc     | binary    | 0.8067   | -0.1016    |

As we can see, each metric used to evaluate the accuracy of our classifier became worse. Both sensitivity (% of true positives) and roc_auc (a measure of true positive rate and false positive rate ) decreased by over 10 percentage points, while specificity (% of true negatives) and accuracy (% correct) decreased by modest amounts. Thus it seems that partitioning the data prior to conducting analysis in this scenario did not improve results, which is not too surprising considering the modified models were given less data to train on than the unmodified ones. Had more observations been in the biomarker dataset, the results may have more closely aligned. Furthermore, the partitioning of the data is done randomly, so perhaps a different seed would have altered the results.

**Modification 2:** Selecting 20 predictive proteins using each selection method.

While the top 10 predictive proteins were selected from each method during the in-class analysis, we will see if selecting 20 proteins instead will help the classifier's accuracy. Below are the results of carrying out this modification:

| Method      | Estimator | Estimate | Difference |
|-------------|-----------|----------|------------|
| sensitivity | Binary    | 0.812    | -0.063     |
| specificity | Binary    | 0.867    | 0.067      |
| accuracy    | Binary    | 0.839    | 0.0003     |
| roc_auc     | Binary    | 0.946    | 0.0377     |

Based on the results of three of the four metrics, selecting 20 of the most important proteins from each selection method improved the classification accuracy, albeit by relatively small margins. Sensitivity was the only metric included in our set that showed a decline in performance when compared to the original classifier, while the overall accuracy increased by 0.03%. In this scenario, a doubling of the amount of top predictive proteins selected from the multiple testing and random forest methods slightly improved results, but it's unclear whether further increases in this amount would help or harm predictive accuracy, as well as what the perfect amount to select from each method would be.

**Modification 3:** Using a fuzzy (instead of hard) intersection to combine the sets of proteins chosen by each selection method.

### Improved classifier

Task 4
